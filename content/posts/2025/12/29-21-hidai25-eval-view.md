---
title: EvalView - AI Agent Testing
date: 2025-12-29 21:06:47 +00:00
tags:
  - hidai25
  - GitHub Actions
draft: false
repo: https://github.com/hidai25/eval-view
marketplace: https://github.com/marketplace/actions/evalview-ai-agent-testing
version: v0.1.7
dependentsNumber: "?"
---


Version updated for **https://github.com/hidai25/eval-view** to version **v0.1.7**.
- This action is used across all versions by **?** repositories.

Go to the [GitHub Marketplace](https://github.com/marketplace/actions/evalview-ai-agent-testing) to find the latest changes.

## Action Summary

EvalView is a testing framework designed for AI agents, enabling developers to write test cases in YAML and automate the detection of regressions in behavior, cost, and latency during CI/CD workflows. By integrating with tools like LangGraph, CrewAI, OpenAI Assistants, and Anthropic Claude, it automates tasks such as tracking token costs, validating tool calls, and catching hallucinations, solving the challenges of manual testing and ensuring reliable agent performance before deployment. Key features include support for regression testing, latency thresholds, and statistical evaluation to address the variability of AI outputs.

## Release notes

## What's New

### Bug Fixes
- **Goose Adapter** - Fixed issues with the Goose adapter integration

### Improvements
- **Skill Doctor** - Now shows example output when no skills are found

**Full Changelog**: https://github.com/hidai25/EvalView/compare/v0.1.6...v0.1.7
